---
permalink: /
title: ""
author_profile: true
redirect_from:  
  - /about/
  - /about.html
---

I am a first-year CS Ph.D. student at the School of Data Science, Chinese University of Hong Kong, Shenzhen (CUHK-SZ), supervised by Prof [Kui Jia](http://kuijia.site/). I received my Bachelor's degree in University of Electronic Science and Technology of China (UESTC, 2014 - 2018), and Master's degree in Nanyang Technological University (NTU, 2018-2019), Singapore. Before joining CUHK-SZ, I worked as a R&D engineer at [DexForce Technology](https://www.dexforce.com/), where I lead the development of DexVerse<sup>TM</sup>, the world's leading Sim2Real AI Platform for embodied intelligence. 

My research interests are mainly in the following areas:
- **Simulation**: Generative model for simulation, Differentiable rendering and physics,  GPU-accelerated simulation.
- **Embodied Intelligence**: Physics-Structured Model Architecture,
 Large-scale model training with online data streams, Sim2Real and domain adaptation, Continual learning and self-evolution of embodied agents.

## Projects {#projects}

<table style="width:100%;max-width:1500px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr bgcolor="#ffffee">
      <td style="padding:5px;width:40%;vertical-align:middle">
        <img src="images/open3d.png" alt="Open3D" width="350" height="350" style="border-style: none">
      </td>
      <td width="60%" valign="middle">
        <a href="https://www.open3d.org/" id="Open3D">
          <span class="papertitle">Open3D: A Modern Library for 3D Data Processing</span>
        </a>
        <br>
        <a href="https://www.open3d.org/">Website</a> | <a href="https://github.com/isl-org/Open3D">Code</a>
        <br>
        <p style="color: #333; margin-top: 0.5em;"><strong style="color: #2f7f93;">Description:</strong> The leading open-source library for 3D processing with 400K+ monthly downloads from PyPI. Open3D exposes a set of carefully selected data structures and algorithms in both C++ and Python for 3D data processing tasks including point cloud processing, mesh processing, and 3D visualization.</p>
      </td>
    </tr>
  </tbody>
</table>


## Publications {#publications}

<table style="width:100%;max-width:1500px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr bgcolor="#efffff">
      <td style="padding:5px;width:40%;vertical-align:middle">
        <img src="images/gs-world.png" alt="GS-World" width="350" height="350" style="border-style: none">
      </td>
      <td width="60%" valign="middle">
        <span class="papertitle" style="color: #555;">GS-World: An Engine-driven Learning Paradigm for Pursuing Embodied Intelligence using World Models of Generative Simulation</span>
        <span style="display:inline-block;background:#ffefc2;color:#6b3b00;padding:3px 8px;border-radius:12px;font-size:0.85em;margin-left:10px;vertical-align:middle;">Position paper</span>
        <br>
        <span style="color: #555;"><a href="http://guiliang.me/">Guiliang Liu</a>, <a href="https://yuecideng.github.io"><strong>Yueci Deng</strong></a>, <a href="https://itszhen.com/"><strong>Zhen Liu</strong></a>,  <a href="http://kuijia.site/">Kui Jia</a></span>
        <br>
        <a href="https://www.researchgate.net/profile/Guiliang-Liu-2/publication/396641390_GS-World_An_Efficient_Engine-driven_Learning_Paradigm_for_Pursuing_Embodied_Intelligence_using_World_Models_of_Generative_Simulation/links/68f33c277d9a4d4e870a9a9d/GS-World-An-Efficient-Engine-driven_Learning_Paradigm-for-Pursuing-Embodied-Intelligence-using-World-Models-of-Generative-Simulation.pdf">Paper</a>
        <br>
      </td>
    </tr>
  </tbody>
</table>   

<table style="width:100%;max-width:1500px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr bgcolor="#efffff">
      <td style="padding:5px;width:40%;vertical-align:middle">
        <img src="images/dexscale.png" alt="DexScale" width="350" height="350" style="border-style: none">
      </td>
      <td width="60%" valign="middle">
        <a href="https://edem-ai.github.io/dexscale.github.io/" id="DexScale">
          <span class="papertitle">DexScale: Automating Data Scaling for Sim2Real Generalizable Robot Control</span>
        </a>
        <br>
        <span style="color: #555;"><a href="http://guiliang.me/">Guiliang Liu</a><sup>*</sup>, <a href="https://yuecideng.github.io"><strong>Yueci Deng</strong></a><sup>*</sup>, <a href="https://github.com/ZhaoRunyi">Runyi Zhao</a>, <a href="https://hnuzhy.github.io/">Huayi Zhou</a>, Jian Chen, Jietao Chen, Ruiyan Xu, Yunxin Tai, <a href="http://kuijia.site/">Kui Jia</a></span>
        <br>
        <small style="color: #888;">(*Equal contribution)</small>
        <br>
        <em style="color: #666;">International Conference on Machine Learning (ICML)</em>, <span style="color: #666;">2025 Poster</span>
        <br>
        <a href="https://openreview.net/pdf?id=AVVXX0erKT">Paper</a> / <span style="color: #888;">Code (Coming Soon)</span>
        <br>
        <p style="color: #333; margin-top: 0.5em;"><strong style="color: #2f7f93;">Description:</strong> A novel data engine for automating data generation and scaling for sim-to-real transfer of robotic manipulation tasks.</p>
      </td>
    </tr>
  </tbody>
</table>   

<table style="width:100%;max-width:1500px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr bgcolor="#efffff">
      <td style="padding:5px;width:40%;vertical-align:middle">
        <img src="images/yoto.png" alt="YOTO" width="300" height="300" style="border-style: none">
      </td>
      <td width="60%" valign="middle">
        <a href="https://hnuzhy.github.io/projects/YOTO" id="DexScale">
          <span class="papertitle">You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations</span>
        </a>
        <br>
        <span style="color: #555;"><a href="https://hnuzhy.github.io/">Huayi Zhou</a>,  <a href="https://openreview.net/profile?id=~Ruixiang_Wang3">Ruixiang Wang</a>, Yunxin Tai, <a href="https://yuecideng.github.io"><strong>Yueci Deng</strong></a>, <a href="http://guiliang.me/">Guiliang Liu</a>, <a href="http://kuijia.site/">Kui Jia</a></span>
        <br>
        <em style="color: #666;">Robotics: Science and Systems (RSS)</em>, <span style="color: #666;">2025</span>
        <br>
        <a href="https://arxiv.org/abs/2501.14208">Paper</a> / <a   href="https://github.com/hnuzhy/YOTO">Code</a>
        <br>
        <p style="color: #333; margin-top: 0.5em;"><strong style="color: #2f7f93;">Description:</strong> This work proposes the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks.</p>
      </td>
    </tr>
  </tbody>
</table>  